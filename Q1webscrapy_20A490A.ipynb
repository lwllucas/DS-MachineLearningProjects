{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Q1webscrapy_20A490A.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPbAJfZ7Dbj1mJ8+bR0hI93",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lwllucas/DS-MachineLearningProjects/blob/master/Q1webscrapy_20A490A.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_rZFRStoGxi",
        "colab_type": "text"
      },
      "source": [
        "## Question 1(6 marks)\n",
        "a)Using python Scrapy module to design a web scraping program to get the\n",
        "content from the following website\n",
        "\n",
        "https://webscraper.io/test-sites/e-commerce/allinone/computers/tablets\n",
        "\n",
        "This is a website for testing web scraping, this given URL link to a computer eCommerce store selling different models of tablets.\n",
        "Your task is to collect information on all the tablets list on the webpage.\n",
        "\n",
        "The information to be collected are product, description, price, and review for each of the tablets. You are supposed to program your code using the focus spiderclass( scrapy.Spider ) in python.\n",
        "\n",
        "The result of the scraped data must store in a JSON format file as listed\n",
        "below:\n",
        "\n",
        "{\"price\": [\"$603.99\"], \"description\": [\"Wi-Fi, 64GB, Silver\"],\"product\": \"Apple iPad Air\", \"review\": \"7 reviews\"}\n",
        "\n",
        "{\"price\": [\"$172.99\"], \"description\": [\"Silver, 7\" IPS, Quad-Core 1.2Ghz, 16GB, 3G, Android 4.2\"], \"product\": \"IdeaTab S5000\", \"review\": \"8 reviews\"}\n",
        "\n",
        "{\"price\": [\"$148.99\"], \"description\": [\"Blue, 7\" IPS, Quad-Core 1.3GHz, 8GB, 3G, Android 4.2\"], \"product\": \"IdeaTab A3500-H\", \"review\": \"9 reviews\"}\n",
        "\n",
        "{\"price\": [\"$233.99\"], \"description\": [\"LTE (SM-T235), Quad-Core 1.2GHz,\n",
        "8GB, Black\"], \"product\": \"Galaxy Tab 4\", \"review\": \"1 reviews\"}\n",
        "\n",
        "{\"price\": [\"$399.99\"], \"description\": [\"10.1\", 3G, Android 4.0, Garnet Red\"], \"product\": \"Galaxy Note\", \"review\": \"12 reviews\"}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7aIG8LQKonX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "3f513e78-e240-48c5-c87c-7bb0c622671b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osjbg3JHoLJX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1c3918af-233e-4160-82e4-7671f4bad9a9"
      },
      "source": [
        "!pip install scrapy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scrapy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d2/b1/105fe9a289e5bb64ec104076546f72060296d9989a0fc31a8b608c810868/Scrapy-2.2.0-py2.py3-none-any.whl (241kB)\n",
            "\r\u001b[K     |█▍                              | 10kB 22.5MB/s eta 0:00:01\r\u001b[K     |██▊                             | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |████                            | 30kB 2.2MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 40kB 2.5MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 51kB 2.0MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 61kB 2.3MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 71kB 2.5MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 81kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 92kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 102kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 112kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 122kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 133kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 143kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 153kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 163kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 174kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 184kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 194kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 204kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 215kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 225kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 235kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 245kB 2.8MB/s \n",
            "\u001b[?25hCollecting parsel>=1.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/23/1e/9b39d64cbab79d4362cdd7be7f5e9623d45c4a53b3f7522cd8210df52d8e/parsel-1.6.0-py2.py3-none-any.whl\n",
            "Collecting pyOpenSSL>=16.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/de/f8342b68fa9e981d348039954657bdf681b2ab93de27443be51865ffa310/pyOpenSSL-19.1.0-py2.py3-none-any.whl (53kB)\n",
            "\r\u001b[K     |██████                          | 10kB 29.8MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 20kB 16.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 30kB 21.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 40kB 14.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 51kB 13.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 6.3MB/s \n",
            "\u001b[?25hCollecting service-identity>=16.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e9/7c/2195b890023e098f9618d43ebc337d83c8b38d414326685339eb024db2f6/service_identity-18.1.0-py2.py3-none-any.whl\n",
            "Collecting w3lib>=1.17.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a3/59/b6b14521090e7f42669cafdb84b0ab89301a42f1f1a82fcf5856661ea3a7/w3lib-1.22.0-py2.py3-none-any.whl\n",
            "Collecting zope.interface>=4.1.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/57/33/565274c28a11af60b7cfc0519d46bde4125fcd7d32ebc0a81b480d0e8da6/zope.interface-5.1.0-cp36-cp36m-manylinux2010_x86_64.whl (234kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 8.8MB/s \n",
            "\u001b[?25hCollecting protego>=0.1.15\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/db/6e/bf6d5e4d7cf233b785719aaec2c38f027b9c2ed980a0015ec1a1cced4893/Protego-0.1.16.tar.gz (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 11.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=3.5.0 in /usr/local/lib/python3.6/dist-packages (from scrapy) (4.2.6)\n",
            "Collecting queuelib>=1.4.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4c/85/ae64e9145f39dd6d14f8af3fa809a270ef3729f3b90b3c0cf5aa242ab0d4/queuelib-1.5.0-py2.py3-none-any.whl\n",
            "Collecting cryptography>=2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/04/686efee2dcdd25aecf357992e7d9362f443eb182ecd623f882bc9f7a6bba/cryptography-2.9.2-cp35-abi3-manylinux2010_x86_64.whl (2.7MB)\n",
            "\u001b[K     |████████████████████████████████| 2.7MB 31.6MB/s \n",
            "\u001b[?25hCollecting PyDispatcher>=2.0.5\n",
            "  Downloading https://files.pythonhosted.org/packages/cd/37/39aca520918ce1935bea9c356bcbb7ed7e52ad4e31bff9b943dfc8e7115b/PyDispatcher-2.0.5.tar.gz\n",
            "Collecting itemadapter>=0.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/7d/fb/92f848fcfa85dc9f95370eaecb5c99b5230dd4fc5c6bae684f4ca59df973/itemadapter-0.1.0-py3-none-any.whl\n",
            "Collecting Twisted>=17.9.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b7/04/1a664c9e5ec0224a1c1a154ddecaa4dc7b8967521bba225efcc41a03d5f3/Twisted-20.3.0-cp36-cp36m-manylinux1_x86_64.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 44.2MB/s \n",
            "\u001b[?25hCollecting cssselect>=0.9.1\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/d4/3b5c17f00cce85b9a1e6f91096e1cc8e8ede2e1be8e96b87ce1ed09e92c5/cssselect-1.1.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from parsel>=1.5.0->scrapy) (1.12.0)\n",
            "Requirement already satisfied: pyasn1 in /usr/local/lib/python3.6/dist-packages (from service-identity>=16.0.0->scrapy) (0.4.8)\n",
            "Requirement already satisfied: attrs>=16.0.0 in /usr/local/lib/python3.6/dist-packages (from service-identity>=16.0.0->scrapy) (19.3.0)\n",
            "Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.6/dist-packages (from service-identity>=16.0.0->scrapy) (0.2.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from zope.interface>=4.1.3->scrapy) (47.3.1)\n",
            "Requirement already satisfied: cffi!=1.11.3,>=1.8 in /usr/local/lib/python3.6/dist-packages (from cryptography>=2.0->scrapy) (1.14.0)\n",
            "Collecting constantly>=15.1\n",
            "  Downloading https://files.pythonhosted.org/packages/b9/65/48c1909d0c0aeae6c10213340ce682db01b48ea900a7d9fce7a7910ff318/constantly-15.1.0-py2.py3-none-any.whl\n",
            "Collecting PyHamcrest!=1.10.0,>=1.9.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/16/e54cc65891f01cb62893540f44ffd3e8dab0a22443e1b438f1a9f5574bee/PyHamcrest-2.0.2-py3-none-any.whl (52kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 5.4MB/s \n",
            "\u001b[?25hCollecting Automat>=0.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/dd/83/5f6f3c1a562674d65efc320257bdc0873ec53147835aeef7762fe7585273/Automat-20.2.0-py2.py3-none-any.whl\n",
            "Collecting incremental>=16.10.1\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/1d/c98a587dc06e107115cf4a58b49de20b19222c83d75335a192052af4c4b7/incremental-17.5.0-py2.py3-none-any.whl\n",
            "Collecting hyperlink>=17.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/7f/91/e916ca10a2de1cb7101a9b24da546fb90ee14629e23160086cf3361c4fb8/hyperlink-19.0.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.0->scrapy) (2.20)\n",
            "Requirement already satisfied: idna>=2.5 in /usr/local/lib/python3.6/dist-packages (from hyperlink>=17.1.1->Twisted>=17.9.0->scrapy) (2.9)\n",
            "Building wheels for collected packages: protego, PyDispatcher\n",
            "  Building wheel for protego (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for protego: filename=Protego-0.1.16-cp36-none-any.whl size=7765 sha256=79a90bc3a9d8b9b910a397c634836eafbc3ad1c2cc0c63838294a4e4c072b5d6\n",
            "  Stored in directory: /root/.cache/pip/wheels/51/01/d1/4a2286a976dccd025ba679acacfe37320540df0f2283ecab12\n",
            "  Building wheel for PyDispatcher (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyDispatcher: filename=PyDispatcher-2.0.5-cp36-none-any.whl size=11515 sha256=93c72d1a20f3bb64adc33f533a33e9f44255f8d460ea9e78a49c74e9548b5e4e\n",
            "  Stored in directory: /root/.cache/pip/wheels/88/99/96/cfef6665f9cb1522ee6757ae5955feedf2fe25f1737f91fa7f\n",
            "Successfully built protego PyDispatcher\n",
            "Installing collected packages: w3lib, cssselect, parsel, cryptography, pyOpenSSL, service-identity, zope.interface, protego, queuelib, PyDispatcher, itemadapter, constantly, PyHamcrest, Automat, incremental, hyperlink, Twisted, scrapy\n",
            "Successfully installed Automat-20.2.0 PyDispatcher-2.0.5 PyHamcrest-2.0.2 Twisted-20.3.0 constantly-15.1.0 cryptography-2.9.2 cssselect-1.1.0 hyperlink-19.0.0 incremental-17.5.0 itemadapter-0.1.0 parsel-1.6.0 protego-0.1.16 pyOpenSSL-19.1.0 queuelib-1.5.0 scrapy-2.2.0 service-identity-18.1.0 w3lib-1.22.0 zope.interface-5.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9JRWECZwWXa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import lxml.etree\n",
        "\n",
        "import json\n",
        "\n",
        "# receives the extract result from the spider and prints out the content\n",
        "class ConsoleWriterPipeline(object):\n",
        "    def open_spider(self, spider):\n",
        "        None\n",
        "    def close_spdier(self, spider):\n",
        "        None\n",
        "    \n",
        "    def process_item(self, item, spider):\n",
        "        line = json.dumps(dict(item)) + \"\\n\"\n",
        "        print(line)\n",
        "        return item\n",
        "\n",
        "# receives the extract result from the spider and appends them into a JSON Line file, (each line is a json)\n",
        "class JsonWriterPipeline(object):\n",
        "    def open_spider(self, spider):\n",
        "        self.file = open(data_dir_path+'result.json', 'w')\n",
        "\n",
        "    def close_spider(self, spider):\n",
        "        print('JSON File Generated')\n",
        "        self.file.close()\n",
        "\n",
        "    def process_item(self, item, spider):\n",
        "        line = json.dumps(dict(item)) + \"\\n\"\n",
        "        self.file.write(line)\n",
        "        return item"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLzZXQBiwh-Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import logging\n",
        "import scrapy\n",
        "from scrapy.crawler import CrawlerProcess"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJKc7E6kKlsM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_dir_path='/content/drive/My Drive/Data/DSA2/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QQWP0N-wlep",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Quote spider is a child from scrapy.spider\n",
        "# standard variables from scrapy.spider are 1.start_urls\n",
        "\n",
        "class QuotesSpider(scrapy.Spider):\n",
        "    name = \"quotes\"\n",
        "    start_urls = [\n",
        "        'https://webscraper.io/test-sites/e-commerce/allinone/computers/tablets'\n",
        "    ]\n",
        "    custom_settings = {\n",
        "        'LOG_LEVEL': logging.WARNING,                            # Default : Debug\n",
        "        #'ITEM_PIPELINES': {'__main__.ConsoleWriterPipeline': 1} # Used for pipeline\n",
        "        'ITEM_PIPELINES': {'__main__.JsonWriterPipeline': 1} # Used for pipeline\n",
        "    }\n",
        "    \n",
        "    def parse(self, response):\n",
        "        for item in response.css('div.thumbnail'):\n",
        "            yield {\n",
        "                'price': [item.css('h4.pull-right.price::text').get()],\n",
        "                'description': [''.join(item.css('p.description::text').re('[^(\\+)]'))],\n",
        "                'product': item.css('a.title::text').get(),\n",
        "                'rating': item.css('p.pull-right::text').get(),\n",
        "            }\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdj2sa_dxBHn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135
        },
        "outputId": "e79ea85b-58c8-41e6-d74f-0c2f3eec18cf"
      },
      "source": [
        "quotes_crawler_process = CrawlerProcess({\n",
        "    'USER_AGENT': 'Mozilla/5.0' #(compatible; MSIE 7.0; Windows NT 5.1)'\n",
        "})\n",
        "\n",
        "quotes_crawler_process.crawl(QuotesSpider)\n",
        "quotes_crawler_process.start()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-06-30 12:37:32 [scrapy.utils.log] INFO: Scrapy 2.2.0 started (bot: scrapybot)\n",
            "2020-06-30 12:37:32 [scrapy.utils.log] INFO: Versions: lxml 4.2.6.0, libxml2 2.9.8, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 20.3.0, Python 3.6.9 (default, Apr 18 2020, 01:56:04) - [GCC 8.4.0], pyOpenSSL 19.1.0 (OpenSSL 1.1.1g  21 Apr 2020), cryptography 2.9.2, Platform Linux-4.19.104+-x86_64-with-Ubuntu-18.04-bionic\n",
            "2020-06-30 12:37:32 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.epollreactor.EPollReactor\n",
            "2020-06-30 12:37:32 [scrapy.crawler] INFO: Overridden settings:\n",
            "{'LOG_LEVEL': 30, 'USER_AGENT': 'Mozilla/5.0'}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "JSON File Generated\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0e6oE9ULjnS",
        "colab_type": "text"
      },
      "source": [
        "b)Read in the collected data in the JSON file.\n",
        "Program a python function to search for Android or Apple Tablet information.\n",
        "\n",
        "Function name: searchbybrand( string)\n",
        "\n",
        "Argument: string -> android or apple\n",
        "\n",
        "Return result: list of all matched item( with product, description, price, reviews)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qtqgp-LfLmN7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "bb04ed65-f102-466a-fda5-df9531ccc2cd"
      },
      "source": [
        "import json\n",
        "\n",
        "def searchbybrand(brand):\n",
        "  for i in range(len(data)):\n",
        "    des2 = data[i]['product']\n",
        "\n",
        "    # Ipad\n",
        "    if brand == 'apple':   \n",
        "      if 'ipad' in des2.lower():\n",
        "        print(des2,data[i]['description'],data[i]['price'], data[i]['rating'])\n",
        "    # Not Ipad\n",
        "    elif brand == 'android':\n",
        "      if 'ipad' not in des2.lower():\n",
        "        print(des2,data[i]['description'],data[i]['price'], data[i]['rating'])\n",
        "\n",
        "data = [json.loads(line) for line in open(data_dir_path + 'result.json', 'r')]\n",
        "# Call function\n",
        "searchbybrand('apple')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iPad Mini Retina ['Wi-Fi  Cellular, 32GB, Silver'] ['$537.99'] 8 reviews\n",
            "Apple iPad Air ['Wi-Fi, 64GB, Silver'] ['$603.99'] 7 reviews\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZvW8L4jMevo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "df742dc2-ef4d-410d-b337-323844eb33c7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6xjntykPqgG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}